# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from transformers import pipeline
from typing import List, Dict

EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)


# -----------------------------
# Load Vector Store
# -----------------------------
def load_vector_store(persist_dir: str = "vector_store"):
    client = chromadb.Client(
        Settings(persist_directory=persist_dir)
    )
    collection = client.get_collection(name="complaints")
    return collection


# -----------------------------
# Retriever
# -----------------------------
def retrieve_chunks(
    question: str,
    collection,
    k: int = 5
) -> List[Dict]:
    query_embedding = embedding_model.encode(question).tolist()

    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=k
    )

    retrieved_docs = []
    for i in range(len(results["documents"][0])):
        retrieved_docs.append({
            "text": results["documents"][0][i],
            "metadata": results["metadatas"][0][i]
        })

    return retrieved_docs

PROMPT_TEMPLATE = """
You are a financial analyst assistant for CrediTrust Financial.

Your task is to answer questions about customer complaints using ONLY the information
provided in the context below. Do not use prior knowledge.
If the context does not contain enough information, say:
"I do not have enough information from customer complaints to answer this question."

Context:
{context}

Question:
{question}

Answer:
"""

# -----------------------------
# Load LLM
# -----------------------------
generator = pipeline(
    "text-generation",
    model="mistralai/Mistral-7B-Instruct-v0.2",
    max_new_tokens=300,
    temperature=0.3
)


def generate_answer(question: str, retrieved_chunks: List[Dict]) -> str:
    context = "\n\n".join([c["text"] for c in retrieved_chunks])

    prompt = PROMPT_TEMPLATE.format(
        context=context,
        question=question
    )

    response = generator(prompt)[0]["generated_text"]
    return response.split("Answer:")[-1].strip()

def rag_pipeline(question: str, k: int = 5):
    collection = load_vector_store()
    retrieved_chunks = retrieve_chunks(question, collection, k)
    answer = generate_answer(question, retrieved_chunks)

    return answer, retrieved_chunks

